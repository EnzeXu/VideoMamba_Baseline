Not using distributed mode
Namespace(batch_size=64, epochs=801, save_ckpt_freq=1000, model='videomamba_middle_pretrain', decoder_depth=4, mask_type='attention', mask_ratio=0.8, input_size=224, drop_path=0.4, normlize_target=True, tubelet_size=1, use_learnable_pos_emb=False, clip_teacher='clip_b16', clip_input_resolution=224, clip_loss_ratio=1.0, clip_loss_type='l2', clip_decoder_type='SA_Decoder', clip_decoder_embed_dim=576, clip_output_dim=512, clip_norm_type='l2', clip_return_attn=True, clip_return_layer=1, clip_return_interval=1.0, clip_student_return_interval=1.0, clip_return_cls=True, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.95], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.00015, warmup_lr=1e-06, min_lr=1e-05, warmup_epochs=40, warmup_steps=-1, use_checkpoint=False, checkpoint_num=0, num_sample=1, color_jitter=0.0, train_interpolation='bicubic', flip='True', prefix='datasets/data/new_k400/kinetics_400/videos_320', split=',', data_path='datasets/data/new_k400/label/train.csv', imagenet_default_mean_and_std=True, use_decord=True, num_segments=8, num_frames=8, sampling_rate=1, output_dir='output/', log_dir='log/', device='cuda', seed=0, resume='', auto_resume=True, start_epoch=0, test_best=False, num_workers=12, pin_mem=True, no_amp=False, bf16=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', distributed=False)
Creating model: videomamba_middle_pretrain
Use checkpoint: False
Checkpoint number: 0
Student return index: [31]
Normalization Type: l2
Patch size = (16, 16)
Tubelet size = 1
Teacher model: clip_b16
Loss ratio: 1.0
Loss type: l2
Normalization Type: l2
Return Attention: True
Return Layer: 1
Return Interval: 1.0
Teacher return index: [11]
load pretrained weights
Inflate: conv1.weight, torch.Size([768, 3, 16, 16]) => torch.Size([768, 3, 1, 16, 16])
Init center: True
Use sparse sampling, change frame and stride
Load dataset using decord: True
Data Aug = (DataAugmentationForVideoMAE,
  transform = Compose(
    <datasets.transforms.GroupMultiScaleCrop object at 0x7f62414147c0>
    <datasets.transforms.GroupRandomHorizontalFlip object at 0x7f62414149d0>
    <datasets.transforms.Stack object at 0x7f6241417fa0>
    <datasets.transforms.ToTorchFormatTensor object at 0x7f6241417fd0>
    <datasets.transforms.GroupNormalize object at 0x7f6241414880>
),
  Masked position generator = None,
)
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f6241417dc0>
Model = VisionMamba(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 576, kernel_size=(1, 16, 16), stride=(1, 16, 16))
  )
  (drop_path): DropPath()
  (layers): ModuleList(
    (0): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
    (1): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
    (2): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (3): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (4): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (5): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (6): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (7): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (8): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (9): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (10): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (11): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (12): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (13): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (14): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (15): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (16): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (17): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (18): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (19): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (20): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (21): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (22): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (23): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (24): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (25): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (26): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (27): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (28): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (29): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (30): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
    (31): Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=576, out_features=2304, bias=False)
        (conv1d): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (act): SiLU()
        (x_proj): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj): Linear(in_features=36, out_features=1152, bias=True)
        (conv1d_b): Conv1d(1152, 1152, kernel_size=(4,), stride=(1,), padding=(3,), groups=1152)
        (x_proj_b): Linear(in_features=1152, out_features=68, bias=False)
        (dt_proj_b): Linear(in_features=36, out_features=1152, bias=True)
        (out_proj): Linear(in_features=1152, out_features=576, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): DropPath()
    )
  )
  (norm): RMSNorm()
  (clip_decoder): ModuleList(
    (0): Linear_Decoder(
      (head): Linear(in_features=576, out_features=512, bias=True)
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
number of params: 73.941504 M
LR = 0.00003750
Batch size = 64
Repeated sample = 1
Number of training steps = 3756
Number of training examples per epoch = 240384
Param groups = {
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "pos_embed",
      "temporal_pos_embedding",
      "patch_embed.proj.bias",
      "layers.0.mixer.D",
      "layers.0.mixer.D_b",
      "layers.0.mixer.conv1d.bias",
      "layers.0.mixer.dt_proj.bias",
      "layers.0.mixer.conv1d_b.bias",
      "layers.0.mixer.dt_proj_b.bias",
      "layers.0.norm.weight",
      "layers.1.mixer.D",
      "layers.1.mixer.D_b",
      "layers.1.mixer.conv1d.bias",
      "layers.1.mixer.dt_proj.bias",
      "layers.1.mixer.conv1d_b.bias",
      "layers.1.mixer.dt_proj_b.bias",
      "layers.1.norm.weight",
      "layers.2.mixer.D",
      "layers.2.mixer.D_b",
      "layers.2.mixer.conv1d.bias",
      "layers.2.mixer.dt_proj.bias",
      "layers.2.mixer.conv1d_b.bias",
      "layers.2.mixer.dt_proj_b.bias",
      "layers.2.norm.weight",
      "layers.3.mixer.D",
      "layers.3.mixer.D_b",
      "layers.3.mixer.conv1d.bias",
      "layers.3.mixer.dt_proj.bias",
      "layers.3.mixer.conv1d_b.bias",
      "layers.3.mixer.dt_proj_b.bias",
      "layers.3.norm.weight",
      "layers.4.mixer.D",
      "layers.4.mixer.D_b",
      "layers.4.mixer.conv1d.bias",
      "layers.4.mixer.dt_proj.bias",
      "layers.4.mixer.conv1d_b.bias",
      "layers.4.mixer.dt_proj_b.bias",
      "layers.4.norm.weight",
      "layers.5.mixer.D",
      "layers.5.mixer.D_b",
      "layers.5.mixer.conv1d.bias",
      "layers.5.mixer.dt_proj.bias",
      "layers.5.mixer.conv1d_b.bias",
      "layers.5.mixer.dt_proj_b.bias",
      "layers.5.norm.weight",
      "layers.6.mixer.D",
      "layers.6.mixer.D_b",
      "layers.6.mixer.conv1d.bias",
      "layers.6.mixer.dt_proj.bias",
      "layers.6.mixer.conv1d_b.bias",
      "layers.6.mixer.dt_proj_b.bias",
      "layers.6.norm.weight",
      "layers.7.mixer.D",
      "layers.7.mixer.D_b",
      "layers.7.mixer.conv1d.bias",
      "layers.7.mixer.dt_proj.bias",
      "layers.7.mixer.conv1d_b.bias",
      "layers.7.mixer.dt_proj_b.bias",
      "layers.7.norm.weight",
      "layers.8.mixer.D",
      "layers.8.mixer.D_b",
      "layers.8.mixer.conv1d.bias",
      "layers.8.mixer.dt_proj.bias",
      "layers.8.mixer.conv1d_b.bias",
      "layers.8.mixer.dt_proj_b.bias",
      "layers.8.norm.weight",
      "layers.9.mixer.D",
      "layers.9.mixer.D_b",
      "layers.9.mixer.conv1d.bias",
      "layers.9.mixer.dt_proj.bias",
      "layers.9.mixer.conv1d_b.bias",
      "layers.9.mixer.dt_proj_b.bias",
      "layers.9.norm.weight",
      "layers.10.mixer.D",
      "layers.10.mixer.D_b",
      "layers.10.mixer.conv1d.bias",
      "layers.10.mixer.dt_proj.bias",
      "layers.10.mixer.conv1d_b.bias",
      "layers.10.mixer.dt_proj_b.bias",
      "layers.10.norm.weight",
      "layers.11.mixer.D",
      "layers.11.mixer.D_b",
      "layers.11.mixer.conv1d.bias",
      "layers.11.mixer.dt_proj.bias",
      "layers.11.mixer.conv1d_b.bias",
      "layers.11.mixer.dt_proj_b.bias",
      "layers.11.norm.weight",
      "layers.12.mixer.D",
      "layers.12.mixer.D_b",
      "layers.12.mixer.conv1d.bias",
      "layers.12.mixer.dt_proj.bias",
      "layers.12.mixer.conv1d_b.bias",
      "layers.12.mixer.dt_proj_b.bias",
      "layers.12.norm.weight",
      "layers.13.mixer.D",
      "layers.13.mixer.D_b",
      "layers.13.mixer.conv1d.bias",
      "layers.13.mixer.dt_proj.bias",
      "layers.13.mixer.conv1d_b.bias",
      "layers.13.mixer.dt_proj_b.bias",
      "layers.13.norm.weight",
      "layers.14.mixer.D",
      "layers.14.mixer.D_b",
      "layers.14.mixer.conv1d.bias",
      "layers.14.mixer.dt_proj.bias",
      "layers.14.mixer.conv1d_b.bias",
      "layers.14.mixer.dt_proj_b.bias",
      "layers.14.norm.weight",
      "layers.15.mixer.D",
      "layers.15.mixer.D_b",
      "layers.15.mixer.conv1d.bias",
      "layers.15.mixer.dt_proj.bias",
      "layers.15.mixer.conv1d_b.bias",
      "layers.15.mixer.dt_proj_b.bias",
      "layers.15.norm.weight",
      "layers.16.mixer.D",
      "layers.16.mixer.D_b",
      "layers.16.mixer.conv1d.bias",
      "layers.16.mixer.dt_proj.bias",
      "layers.16.mixer.conv1d_b.bias",
      "layers.16.mixer.dt_proj_b.bias",
      "layers.16.norm.weight",
      "layers.17.mixer.D",
      "layers.17.mixer.D_b",
      "layers.17.mixer.conv1d.bias",
      "layers.17.mixer.dt_proj.bias",
      "layers.17.mixer.conv1d_b.bias",
      "layers.17.mixer.dt_proj_b.bias",
      "layers.17.norm.weight",
      "layers.18.mixer.D",
      "layers.18.mixer.D_b",
      "layers.18.mixer.conv1d.bias",
      "layers.18.mixer.dt_proj.bias",
      "layers.18.mixer.conv1d_b.bias",
      "layers.18.mixer.dt_proj_b.bias",
      "layers.18.norm.weight",
      "layers.19.mixer.D",
      "layers.19.mixer.D_b",
      "layers.19.mixer.conv1d.bias",
      "layers.19.mixer.dt_proj.bias",
      "layers.19.mixer.conv1d_b.bias",
      "layers.19.mixer.dt_proj_b.bias",
      "layers.19.norm.weight",
      "layers.20.mixer.D",
      "layers.20.mixer.D_b",
      "layers.20.mixer.conv1d.bias",
      "layers.20.mixer.dt_proj.bias",
      "layers.20.mixer.conv1d_b.bias",
      "layers.20.mixer.dt_proj_b.bias",
      "layers.20.norm.weight",
      "layers.21.mixer.D",
      "layers.21.mixer.D_b",
      "layers.21.mixer.conv1d.bias",
      "layers.21.mixer.dt_proj.bias",
      "layers.21.mixer.conv1d_b.bias",
      "layers.21.mixer.dt_proj_b.bias",
      "layers.21.norm.weight",
      "layers.22.mixer.D",
      "layers.22.mixer.D_b",
      "layers.22.mixer.conv1d.bias",
      "layers.22.mixer.dt_proj.bias",
      "layers.22.mixer.conv1d_b.bias",
      "layers.22.mixer.dt_proj_b.bias",
      "layers.22.norm.weight",
      "layers.23.mixer.D",
      "layers.23.mixer.D_b",
      "layers.23.mixer.conv1d.bias",
      "layers.23.mixer.dt_proj.bias",
      "layers.23.mixer.conv1d_b.bias",
      "layers.23.mixer.dt_proj_b.bias",
      "layers.23.norm.weight",
      "layers.24.mixer.D",
      "layers.24.mixer.D_b",
      "layers.24.mixer.conv1d.bias",
      "layers.24.mixer.dt_proj.bias",
      "layers.24.mixer.conv1d_b.bias",
      "layers.24.mixer.dt_proj_b.bias",
      "layers.24.norm.weight",
      "layers.25.mixer.D",
      "layers.25.mixer.D_b",
      "layers.25.mixer.conv1d.bias",
      "layers.25.mixer.dt_proj.bias",
      "layers.25.mixer.conv1d_b.bias",
      "layers.25.mixer.dt_proj_b.bias",
      "layers.25.norm.weight",
      "layers.26.mixer.D",
      "layers.26.mixer.D_b",
      "layers.26.mixer.conv1d.bias",
      "layers.26.mixer.dt_proj.bias",
      "layers.26.mixer.conv1d_b.bias",
      "layers.26.mixer.dt_proj_b.bias",
      "layers.26.norm.weight",
      "layers.27.mixer.D",
      "layers.27.mixer.D_b",
      "layers.27.mixer.conv1d.bias",
      "layers.27.mixer.dt_proj.bias",
      "layers.27.mixer.conv1d_b.bias",
      "layers.27.mixer.dt_proj_b.bias",
      "layers.27.norm.weight",
      "layers.28.mixer.D",
      "layers.28.mixer.D_b",
      "layers.28.mixer.conv1d.bias",
      "layers.28.mixer.dt_proj.bias",
      "layers.28.mixer.conv1d_b.bias",
      "layers.28.mixer.dt_proj_b.bias",
      "layers.28.norm.weight",
      "layers.29.mixer.D",
      "layers.29.mixer.D_b",
      "layers.29.mixer.conv1d.bias",
      "layers.29.mixer.dt_proj.bias",
      "layers.29.mixer.conv1d_b.bias",
      "layers.29.mixer.dt_proj_b.bias",
      "layers.29.norm.weight",
      "layers.30.mixer.D",
      "layers.30.mixer.D_b",
      "layers.30.mixer.conv1d.bias",
      "layers.30.mixer.dt_proj.bias",
      "layers.30.mixer.conv1d_b.bias",
      "layers.30.mixer.dt_proj_b.bias",
      "layers.30.norm.weight",
      "layers.31.mixer.D",
      "layers.31.mixer.D_b",
      "layers.31.mixer.conv1d.bias",
      "layers.31.mixer.dt_proj.bias",
      "layers.31.mixer.conv1d_b.bias",
      "layers.31.mixer.dt_proj_b.bias",
      "layers.31.norm.weight",
      "norm.weight",
      "clip_decoder.0.head.bias",
      "clip_decoder.0.norm.weight",
      "clip_decoder.0.norm.bias"
    ],
    "lr_scale": 1.0
  },
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight",
      "layers.0.mixer.A_log",
      "layers.0.mixer.A_b_log",
      "layers.0.mixer.in_proj.weight",
      "layers.0.mixer.conv1d.weight",
      "layers.0.mixer.x_proj.weight",
      "layers.0.mixer.dt_proj.weight",
      "layers.0.mixer.conv1d_b.weight",
      "layers.0.mixer.x_proj_b.weight",
      "layers.0.mixer.dt_proj_b.weight",
      "layers.0.mixer.out_proj.weight",
      "layers.1.mixer.A_log",
      "layers.1.mixer.A_b_log",
      "layers.1.mixer.in_proj.weight",
      "layers.1.mixer.conv1d.weight",
      "layers.1.mixer.x_proj.weight",
      "layers.1.mixer.dt_proj.weight",
      "layers.1.mixer.conv1d_b.weight",
      "layers.1.mixer.x_proj_b.weight",
      "layers.1.mixer.dt_proj_b.weight",
      "layers.1.mixer.out_proj.weight",
      "layers.2.mixer.A_log",
      "layers.2.mixer.A_b_log",
      "layers.2.mixer.in_proj.weight",
      "layers.2.mixer.conv1d.weight",
      "layers.2.mixer.x_proj.weight",
      "layers.2.mixer.dt_proj.weight",
      "layers.2.mixer.conv1d_b.weight",
      "layers.2.mixer.x_proj_b.weight",
      "layers.2.mixer.dt_proj_b.weight",
      "layers.2.mixer.out_proj.weight",
      "layers.3.mixer.A_log",
      "layers.3.mixer.A_b_log",
      "layers.3.mixer.in_proj.weight",
      "layers.3.mixer.conv1d.weight",
      "layers.3.mixer.x_proj.weight",
      "layers.3.mixer.dt_proj.weight",
      "layers.3.mixer.conv1d_b.weight",
      "layers.3.mixer.x_proj_b.weight",
      "layers.3.mixer.dt_proj_b.weight",
      "layers.3.mixer.out_proj.weight",
      "layers.4.mixer.A_log",
      "layers.4.mixer.A_b_log",
      "layers.4.mixer.in_proj.weight",
      "layers.4.mixer.conv1d.weight",
      "layers.4.mixer.x_proj.weight",
      "layers.4.mixer.dt_proj.weight",
      "layers.4.mixer.conv1d_b.weight",
      "layers.4.mixer.x_proj_b.weight",
      "layers.4.mixer.dt_proj_b.weight",
      "layers.4.mixer.out_proj.weight",
      "layers.5.mixer.A_log",
      "layers.5.mixer.A_b_log",
      "layers.5.mixer.in_proj.weight",
      "layers.5.mixer.conv1d.weight",
      "layers.5.mixer.x_proj.weight",
      "layers.5.mixer.dt_proj.weight",
      "layers.5.mixer.conv1d_b.weight",
      "layers.5.mixer.x_proj_b.weight",
      "layers.5.mixer.dt_proj_b.weight",
      "layers.5.mixer.out_proj.weight",
      "layers.6.mixer.A_log",
      "layers.6.mixer.A_b_log",
      "layers.6.mixer.in_proj.weight",
      "layers.6.mixer.conv1d.weight",
      "layers.6.mixer.x_proj.weight",
      "layers.6.mixer.dt_proj.weight",
      "layers.6.mixer.conv1d_b.weight",
      "layers.6.mixer.x_proj_b.weight",
      "layers.6.mixer.dt_proj_b.weight",
      "layers.6.mixer.out_proj.weight",
      "layers.7.mixer.A_log",
      "layers.7.mixer.A_b_log",
      "layers.7.mixer.in_proj.weight",
      "layers.7.mixer.conv1d.weight",
      "layers.7.mixer.x_proj.weight",
      "layers.7.mixer.dt_proj.weight",
      "layers.7.mixer.conv1d_b.weight",
      "layers.7.mixer.x_proj_b.weight",
      "layers.7.mixer.dt_proj_b.weight",
      "layers.7.mixer.out_proj.weight",
      "layers.8.mixer.A_log",
      "layers.8.mixer.A_b_log",
      "layers.8.mixer.in_proj.weight",
      "layers.8.mixer.conv1d.weight",
      "layers.8.mixer.x_proj.weight",
      "layers.8.mixer.dt_proj.weight",
      "layers.8.mixer.conv1d_b.weight",
      "layers.8.mixer.x_proj_b.weight",
      "layers.8.mixer.dt_proj_b.weight",
      "layers.8.mixer.out_proj.weight",
      "layers.9.mixer.A_log",
      "layers.9.mixer.A_b_log",
      "layers.9.mixer.in_proj.weight",
      "layers.9.mixer.conv1d.weight",
      "layers.9.mixer.x_proj.weight",
      "layers.9.mixer.dt_proj.weight",
      "layers.9.mixer.conv1d_b.weight",
      "layers.9.mixer.x_proj_b.weight",
      "layers.9.mixer.dt_proj_b.weight",
      "layers.9.mixer.out_proj.weight",
      "layers.10.mixer.A_log",
      "layers.10.mixer.A_b_log",
      "layers.10.mixer.in_proj.weight",
      "layers.10.mixer.conv1d.weight",
      "layers.10.mixer.x_proj.weight",
      "layers.10.mixer.dt_proj.weight",
      "layers.10.mixer.conv1d_b.weight",
      "layers.10.mixer.x_proj_b.weight",
      "layers.10.mixer.dt_proj_b.weight",
      "layers.10.mixer.out_proj.weight",
      "layers.11.mixer.A_log",
      "layers.11.mixer.A_b_log",
      "layers.11.mixer.in_proj.weight",
      "layers.11.mixer.conv1d.weight",
      "layers.11.mixer.x_proj.weight",
      "layers.11.mixer.dt_proj.weight",
      "layers.11.mixer.conv1d_b.weight",
      "layers.11.mixer.x_proj_b.weight",
      "layers.11.mixer.dt_proj_b.weight",
      "layers.11.mixer.out_proj.weight",
      "layers.12.mixer.A_log",
      "layers.12.mixer.A_b_log",
      "layers.12.mixer.in_proj.weight",
      "layers.12.mixer.conv1d.weight",
      "layers.12.mixer.x_proj.weight",
      "layers.12.mixer.dt_proj.weight",
      "layers.12.mixer.conv1d_b.weight",
      "layers.12.mixer.x_proj_b.weight",
      "layers.12.mixer.dt_proj_b.weight",
      "layers.12.mixer.out_proj.weight",
      "layers.13.mixer.A_log",
      "layers.13.mixer.A_b_log",
      "layers.13.mixer.in_proj.weight",
      "layers.13.mixer.conv1d.weight",
      "layers.13.mixer.x_proj.weight",
      "layers.13.mixer.dt_proj.weight",
      "layers.13.mixer.conv1d_b.weight",
      "layers.13.mixer.x_proj_b.weight",
      "layers.13.mixer.dt_proj_b.weight",
      "layers.13.mixer.out_proj.weight",
      "layers.14.mixer.A_log",
      "layers.14.mixer.A_b_log",
      "layers.14.mixer.in_proj.weight",
      "layers.14.mixer.conv1d.weight",
      "layers.14.mixer.x_proj.weight",
      "layers.14.mixer.dt_proj.weight",
      "layers.14.mixer.conv1d_b.weight",
      "layers.14.mixer.x_proj_b.weight",
      "layers.14.mixer.dt_proj_b.weight",
      "layers.14.mixer.out_proj.weight",
      "layers.15.mixer.A_log",
      "layers.15.mixer.A_b_log",
      "layers.15.mixer.in_proj.weight",
      "layers.15.mixer.conv1d.weight",
      "layers.15.mixer.x_proj.weight",
      "layers.15.mixer.dt_proj.weight",
      "layers.15.mixer.conv1d_b.weight",
      "layers.15.mixer.x_proj_b.weight",
      "layers.15.mixer.dt_proj_b.weight",
      "layers.15.mixer.out_proj.weight",
      "layers.16.mixer.A_log",
      "layers.16.mixer.A_b_log",
      "layers.16.mixer.in_proj.weight",
      "layers.16.mixer.conv1d.weight",
      "layers.16.mixer.x_proj.weight",
      "layers.16.mixer.dt_proj.weight",
      "layers.16.mixer.conv1d_b.weight",
      "layers.16.mixer.x_proj_b.weight",
      "layers.16.mixer.dt_proj_b.weight",
      "layers.16.mixer.out_proj.weight",
      "layers.17.mixer.A_log",
      "layers.17.mixer.A_b_log",
      "layers.17.mixer.in_proj.weight",
      "layers.17.mixer.conv1d.weight",
      "layers.17.mixer.x_proj.weight",
      "layers.17.mixer.dt_proj.weight",
      "layers.17.mixer.conv1d_b.weight",
      "layers.17.mixer.x_proj_b.weight",
      "layers.17.mixer.dt_proj_b.weight",
      "layers.17.mixer.out_proj.weight",
      "layers.18.mixer.A_log",
      "layers.18.mixer.A_b_log",
      "layers.18.mixer.in_proj.weight",
      "layers.18.mixer.conv1d.weight",
      "layers.18.mixer.x_proj.weight",
      "layers.18.mixer.dt_proj.weight",
      "layers.18.mixer.conv1d_b.weight",
      "layers.18.mixer.x_proj_b.weight",
      "layers.18.mixer.dt_proj_b.weight",
      "layers.18.mixer.out_proj.weight",
      "layers.19.mixer.A_log",
      "layers.19.mixer.A_b_log",
      "layers.19.mixer.in_proj.weight",
      "layers.19.mixer.conv1d.weight",
      "layers.19.mixer.x_proj.weight",
      "layers.19.mixer.dt_proj.weight",
      "layers.19.mixer.conv1d_b.weight",
      "layers.19.mixer.x_proj_b.weight",
      "layers.19.mixer.dt_proj_b.weight",
      "layers.19.mixer.out_proj.weight",
      "layers.20.mixer.A_log",
      "layers.20.mixer.A_b_log",
      "layers.20.mixer.in_proj.weight",
      "layers.20.mixer.conv1d.weight",
      "layers.20.mixer.x_proj.weight",
      "layers.20.mixer.dt_proj.weight",
      "layers.20.mixer.conv1d_b.weight",
      "layers.20.mixer.x_proj_b.weight",
      "layers.20.mixer.dt_proj_b.weight",
      "layers.20.mixer.out_proj.weight",
      "layers.21.mixer.A_log",
      "layers.21.mixer.A_b_log",
      "layers.21.mixer.in_proj.weight",
      "layers.21.mixer.conv1d.weight",
      "layers.21.mixer.x_proj.weight",
      "layers.21.mixer.dt_proj.weight",
      "layers.21.mixer.conv1d_b.weight",
      "layers.21.mixer.x_proj_b.weight",
      "layers.21.mixer.dt_proj_b.weight",
      "layers.21.mixer.out_proj.weight",
      "layers.22.mixer.A_log",
      "layers.22.mixer.A_b_log",
      "layers.22.mixer.in_proj.weight",
      "layers.22.mixer.conv1d.weight",
      "layers.22.mixer.x_proj.weight",
      "layers.22.mixer.dt_proj.weight",
      "layers.22.mixer.conv1d_b.weight",
      "layers.22.mixer.x_proj_b.weight",
      "layers.22.mixer.dt_proj_b.weight",
      "layers.22.mixer.out_proj.weight",
      "layers.23.mixer.A_log",
      "layers.23.mixer.A_b_log",
      "layers.23.mixer.in_proj.weight",
      "layers.23.mixer.conv1d.weight",
      "layers.23.mixer.x_proj.weight",
      "layers.23.mixer.dt_proj.weight",
      "layers.23.mixer.conv1d_b.weight",
      "layers.23.mixer.x_proj_b.weight",
      "layers.23.mixer.dt_proj_b.weight",
      "layers.23.mixer.out_proj.weight",
      "layers.24.mixer.A_log",
      "layers.24.mixer.A_b_log",
      "layers.24.mixer.in_proj.weight",
      "layers.24.mixer.conv1d.weight",
      "layers.24.mixer.x_proj.weight",
      "layers.24.mixer.dt_proj.weight",
      "layers.24.mixer.conv1d_b.weight",
      "layers.24.mixer.x_proj_b.weight",
      "layers.24.mixer.dt_proj_b.weight",
      "layers.24.mixer.out_proj.weight",
      "layers.25.mixer.A_log",
      "layers.25.mixer.A_b_log",
      "layers.25.mixer.in_proj.weight",
      "layers.25.mixer.conv1d.weight",
      "layers.25.mixer.x_proj.weight",
      "layers.25.mixer.dt_proj.weight",
      "layers.25.mixer.conv1d_b.weight",
      "layers.25.mixer.x_proj_b.weight",
      "layers.25.mixer.dt_proj_b.weight",
      "layers.25.mixer.out_proj.weight",
      "layers.26.mixer.A_log",
      "layers.26.mixer.A_b_log",
      "layers.26.mixer.in_proj.weight",
      "layers.26.mixer.conv1d.weight",
      "layers.26.mixer.x_proj.weight",
      "layers.26.mixer.dt_proj.weight",
      "layers.26.mixer.conv1d_b.weight",
      "layers.26.mixer.x_proj_b.weight",
      "layers.26.mixer.dt_proj_b.weight",
      "layers.26.mixer.out_proj.weight",
      "layers.27.mixer.A_log",
      "layers.27.mixer.A_b_log",
      "layers.27.mixer.in_proj.weight",
      "layers.27.mixer.conv1d.weight",
      "layers.27.mixer.x_proj.weight",
      "layers.27.mixer.dt_proj.weight",
      "layers.27.mixer.conv1d_b.weight",
      "layers.27.mixer.x_proj_b.weight",
      "layers.27.mixer.dt_proj_b.weight",
      "layers.27.mixer.out_proj.weight",
      "layers.28.mixer.A_log",
      "layers.28.mixer.A_b_log",
      "layers.28.mixer.in_proj.weight",
      "layers.28.mixer.conv1d.weight",
      "layers.28.mixer.x_proj.weight",
      "layers.28.mixer.dt_proj.weight",
      "layers.28.mixer.conv1d_b.weight",
      "layers.28.mixer.x_proj_b.weight",
      "layers.28.mixer.dt_proj_b.weight",
      "layers.28.mixer.out_proj.weight",
      "layers.29.mixer.A_log",
      "layers.29.mixer.A_b_log",
      "layers.29.mixer.in_proj.weight",
      "layers.29.mixer.conv1d.weight",
      "layers.29.mixer.x_proj.weight",
      "layers.29.mixer.dt_proj.weight",
      "layers.29.mixer.conv1d_b.weight",
      "layers.29.mixer.x_proj_b.weight",
      "layers.29.mixer.dt_proj_b.weight",
      "layers.29.mixer.out_proj.weight",
      "layers.30.mixer.A_log",
      "layers.30.mixer.A_b_log",
      "layers.30.mixer.in_proj.weight",
      "layers.30.mixer.conv1d.weight",
      "layers.30.mixer.x_proj.weight",
      "layers.30.mixer.dt_proj.weight",
      "layers.30.mixer.conv1d_b.weight",
      "layers.30.mixer.x_proj_b.weight",
      "layers.30.mixer.dt_proj_b.weight",
      "layers.30.mixer.out_proj.weight",
      "layers.31.mixer.A_log",
      "layers.31.mixer.A_b_log",
      "layers.31.mixer.in_proj.weight",
      "layers.31.mixer.conv1d.weight",
      "layers.31.mixer.x_proj.weight",
      "layers.31.mixer.dt_proj.weight",
      "layers.31.mixer.conv1d_b.weight",
      "layers.31.mixer.x_proj_b.weight",
      "layers.31.mixer.dt_proj_b.weight",
      "layers.31.mixer.out_proj.weight",
      "clip_decoder.0.head.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 3.75e-05, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.95]}
Use bf16: True
Use step level LR & WD scheduler!
Set warmup steps = 150240
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Auto resume checkpoint: 
Start training for 801 epochs
Mask ratio: 0.8
